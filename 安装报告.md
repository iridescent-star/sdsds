# [物理分布式环境安装实验报告]()

组员：柴百里、陈杨平、张爽、王宇春、魏国太、叶浩祥、吴振华



### 安装前的准备

安装物理分布式的前提首先是配置好hadoop和spark的伪分布式环境，这个用Ta给出的安装文档就行了，提前准备好必须的java环境和环境变量。



为了方便统一，我们使用了柴百里同学的手机热点，然后给每个组员分配了新的主机名、IP、角色

（以下IP为最后一次一起做实验的IP）

|       IP       | 主机名  | 角色（序号） |
| :------------: | :-----: | :----------: |
| 192.168.43.233 | chaibli |    master    |
| 192.168.43.69  |   zs    |   worker1    |
| 192.168.43.125 |  chun   |   worker2    |
| 192.168.43.215 |   tai   |   worker3    |
| 192.168.43.106 |   ye    |   worker4    |
| 192.168.43.178 |   wu    |   worker5    |



### Hadoop安装



#### 一、添加hosts主机名映射，六个节点的名称与对应的 IP

在每台机器上都要去添加

```text
vi /etc/hosts
```



在文件第一行开始添加如下信息：

```
 192.168.43.233 chaibli  #master  
 192.168.43.69     zs    #worker1 
 192.168.43.125   chun   #worker2 
 192.168.43.215    tai   #worker3 
 192.168.43.106    ye    #worker4 
 192.168.43.178    wu    #worker5 
```



#### 二、生成秘钥()

#### 1、在Master机器上执行

```text
su - hadoop
cd ~/.ssh               # 如果没有该目录，先执行一次ssh localhost
rm ./id_rsa*            # 删除之前生成的公匙（如果有）
ssh-keygen -t rsa       # 一直按回车就可以

#让 Master节点也能无密码 SSH 本机
cat ./id_rsa.pub >> ./authorized_keys 

#完成后可执行ssh Master验证一下（可能需要输入 yes，成功后执行 exit 返回原来的终端）。
ssh -p 2112 chaibli 

#在Master节点将上公匙传输到各个节点
scp ~/.ssh/id_rsa.pub zs:/usr/local/hadoop
scp ~/.ssh/id_rsa.pub chun:/usr/local/hadoop
scp ~/.ssh/id_rsa.pub tai:/usr/local/hadoop
scp ~/.ssh/id_rsa.pub ye:/usr/local/hadoop
scp ~/.ssh/id_rsa.pub wu:/usr/local/hadoop
```

#### 2、在worker机器执行

```text
su - hadoop
mkdir .ssh # 如果没有该目录，则新建.ssh目录
cat id_rsa.pub >> ~/.ssh/authorized_keys
rm id_rsa.pub
su - root 
chmod 700 -R /usr/local/hadoop
chmod 644 /usr/local/hadoop/.ssh/authorized_keys
```

注意$HOME/.ssh目录 或 $HOME目录的权限 最好是700，我就在这里栽跟头了。 注意uthorized_keys的权限 chmod 644 authorized_keys 这个也是要注意的。



#### 三、在Master机器执行验证

```text
su - hadoop
ssh -p 2112 zs
ssh -p 2112 chun
ssh -p 2112 tai
ssh -p 2112 ye
ssh -p 2112 wu
```



#### 四、配置集群环境

集群/分布式模式需要修改 /usr/local/hadoop/etc/hadoop 中的5个配置文件，更多设置项可点击查看官方说明，这里仅设置了正常启动所必须的设置项： workers、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml 。

#### 1、文件 workers

将作为 DataNode 的主机名写入该文件，每行一个，默认为 localhost，所以在伪分布式配置时，节点即作为 NameNode 也作为 DataNode。分布式配置可以保留 localhost，也可以删掉，让 Master 节点仅作为 NameNode 使用。本教程让Master节点仅作为 NameNode 使用，因此将文件中原来的 localhost 删除，只添加

其它的worker节点

```text
vi workers
```

添加如下内容

```text
zs
chun
tai
ye
wu
```



#### 2、文件core-site.xml 改为下面的配置：

```xml
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/home/hadoop/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>chaibli:9000</value>
    </property>
</configuration>
```



#### 3、文件 hdfs-site.xml，dfs.replication 一般设为 3，但我们有5个 Worker 节点，所以 dfs.replication 的值还是设为5：

```xml
<configuration>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>chaibli:50090</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>5</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/home/hadoop/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/home/hadoop/tmp/dfs/data</value>
    </property>
</configuration>
```



#### 4、文件 mapred-site.xml （可能需要先重命名，默认文件名为 mapred-site.xml.template），然后配置修改如下：

```xml
<configuration>
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.address</name>
                <value>chaibli:10020</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.webapp.address</name>
                <value>chaibli:19888</value>
        </property>
        <property>
                <name>mapreduce.admin.user.env</name>
                <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
        </property>
        <property>
                <name>yarn.app.mapreduce.am.env</name>
                <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
        </property>
</configuration>
```



#### 5、文件 yarn-site.xml：

```xml
<configuration>
        <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>chaibli</value>
        </property>
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
</configuration>
```

配置好后，将Master上的 /opt/Hadoop 文件夹复制到各个节点上。

在Master节点上执行：

```bash
su - root
cd /usr/local
sudo rm -r ./hadoop/tmp     # 删除 Hadoop 临时文件
sudo rm -r ./hadoop/logs/*   # 删除日志文件
tar -zcvf hadoop.master.tar.gz hadoop   # 先压缩再复制
mv hadoop.master.tar.gz /home/hadoop
chown -R hadoop:hadoop /home/hadoop
su - hadoop
scp hadoop.master.tar.gz zs:/home/hadoop
scp hadoop.master.tar.gz chun:/home/hadoop
scp hadoop.master.tar.gz tai:/home/hadoop
scp hadoop.master.tar.gz ye:/home/hadoop
scp hadoop.master.tar.gz wu:/home/hadoop
```

在zs节点上执行：

```text
su - root
sudo tar -zxvf /home/hadoop/hadoop.master.tar.gz -C /usr/local/hadoop
sudo chown -R hadoop /usr/local/hadoop
```

同样，如果有其他 Worker 节点，也要执行将 hadoop.master.tar.gz 传输到 Worker 节点、在 Worker 节点解压文件的操作。



#### 六、格式化

首次启动需要先在 Master 节点执行 NameNode 的格式化：

```text
hdfs namenode -format  #执行初始化
```



#### 七、启动hadoop

启动需要在 Master 节点上进行

```text
start-dfs.sh
start-yarn.sh
mapred --daemon start historyserver
mr-jobhistory-daemon.sh start historyserver
```

 通过命令 `jps` 可以查看各个节点所启动的进程。正确的话，在 Master 节点上可以看到 NameNode、ResourceManager、SecondrryNameNode、JobHistoryServer 进程 (**顺序不固定**)



![8](C:\Users\Chester\Desktop\张爽的study\专必\机器学习\spark and hadoop\work2\8.png)



在 Worker 节点可以看到 DataNode 和 NodeManager 进程，如下图所示：      

​                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   

![20](C:\Users\Chester\Desktop\张爽的study\专必\机器学习\spark and hadoop\work2\20.png)



缺少任一进程都表示出错。另外还需要在 Master 节点上通过命令 `hdfs dfsadmin -report` 查看 DataNode 是否正常启动，如果 Live datanodes 不为 0 ，则说明集群启动成功。例如我这边一共有 1 个 Datanodes：



![2](C:\Users\Chester\Desktop\张爽的study\专必\机器学习\spark and hadoop\work2\2.png)



![12](C:\Users\Chester\Desktop\张爽的study\专必\机器学习\spark and hadoop\work2\12.png)

​                                                                      （**这是三个节点的连接情况**）



#### 八、执行分布式实例

执行分布式实例过程与伪分布式模式一样，首先创建 HDFS 上的用户目录：

```bash
hdfs dfs -mkdir -p /user/hadoop
```

Shell 命令

将 /usr/local/hadoop/etc/hadoop 中的配置文件作为输入文件复制到分布式文件系统中：

```bash
hdfs dfs -mkdir inputhdfs dfs -put /usr/local/hadoop/etc/hadoop/*.xml input
```

Shell 命令

通过查看 DataNode 的状态（占用大小有改变），输入文件确实复制到了 DataNode 中，如下图所示：

然后登录chaibli:9870，查看节点信息，发现登录：



![4](C:\Users\Chester\Desktop\张爽的study\专必\机器学习\spark and hadoop\work2\4.png)



**接着就可以运行 MapReduce 作业了：**

```
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep input output 'dfs[a-z.]+'
```



![14](C:\Users\Chester\Desktop\张爽的study\专必\机器学习\spark and hadoop\work2\spark.png)



![17](C:\Users\Chester\Desktop\张爽的study\专必\机器学习\spark and hadoop\work2\17.png)



运行时的输出信息与伪分布式类似，会显示 Job 的进度。

然后我们可以发现最终MapReduce任务完成度达到了100%，作业完成。



接着再输入

```
hdfs dfs -cat output/*
```

然后我们可以看到执行完毕后的输出结果：



![18](C:\Users\Chester\Desktop\张爽的study\专必\机器学习\spark and hadoop\work2\18.png)



自此， Hadoop 的集群成功搭建。





### Spark安装

Spark是运行在hadoop集群上的，成功安装好了物理分布式的hadoop之后，安装就会轻松很多



进入/usr/local/spark/conf

复制 spark-env.sh.template 成 spark-env.sh

```text
cp spark-env.sh.template spark-env.sh
```

修改$SPARK_HOME/conf/spark-env.sh，添加如下内容：

```text
export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_162
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
export SPARK_MASTER_IP=chaibli #主机master的ip地址
export SPARK_MASTER_HOST=chaibli
export SPARK_LOCAL_IP=chaibli
export SPARK_WORKER_MEMORY=2g #运行内存，视情况而变
export SPARK_WORKER_CORES=5 #五个slave
export SPARK_HOME=/opt/spark-2.4.0-bin-hadoop2.7
export SPARK_DIST_CLASSPATH=$(/opt/hadoop-2.8.5/bin/hadoop classpath)
```

复制$SPARK_HOME/conf/slaves目录下的slaves.template成slaves

```text
cp slaves.template slaves
```

然后添加如下内容：

```text
master
slave1
slave2
```

将配置好的spark文件复制到所有Slave节点。

```text
scp /usr/local/spark zs:/usr/local/spark
scp /usr/local/spark chun:/usr/local/spark
scp /usr/local/spark tai:/usr/local/spark
scp /usr/local/spark ye:/usr/local/spark
scp /usr/local/spark wu:/usr/local/spark
```



在所有Slave中修改$SPARK_HOME/conf/spark-env.sh，将export SPARK_LOCAL_IP改成Slave对应节点的IP，然后再source一下环境变量。



进入/usr/local/spark 目录下用./sbin/start-all.sh启动spark:



![13](C:\Users\Chester\Desktop\张爽的study\专必\机器学习\spark and hadoop\work2\13.png)



登录chaibli:7077，我们可以看到以下节点信息：



![7](C:\Users\Chester\Desktop\张爽的study\专必\机器学习\spark and hadoop\work2\7.png)

我们最后可以通过登录，发现任务完成

```
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://chaibli:7077 \
  examples/jars/spark-examples_2.12-3.0.2.jar \
  100
```



![spark](C:\Users\Chester\Desktop\张爽的study\专必\机器学习\spark and hadoop\work2\spark.png)



```
bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn examples/jars/spark-examples_2.12-3.0.2.jar

```



![19](C:\Users\Chester\Desktop\张爽的study\专必\机器学习\spark and hadoop\work2\19.png)  





### 实验心得：

​		本次的物理分布式安装是一次奇特的体验，甚至可以用玄学来形容。

​    	一开始每个我们采用的是校园网，但是在我们关闭防火墙之后，虚拟机之间的ping还是会有一定程度上的连接问题，所以改成了手机热点。跑实验的时候，也没有想象中的那么顺利，有的时候删了临时文件才能运行，有的时候连接不上那就格式化，有的时候这两个都不执行，任务居然能比较顺畅的完成。

​		中途查了很多资料，也踩了很多坑，犯了很多小的错误——比方说删除临时文件的时候把整个hadoop文件夹给删掉了，或者是在etc/host里面的ip地址的点敲成了逗号......

​		我们遇到最大的问题就是在跑MapReduce作业的时候会卡住，完成情况一再卡在67%，上网搜索发现说可能是虚拟机默认的配置空间不够，进入配置文件修改之后也没能很好解决；我们又去查看运行的日志，发现出现了挺多java.net.unknownhostexception的错误，但也未能得到很好解决。不过，我们改变了连接节点的运行情况的时候，任务可以比较顺利的执行，我们也挺疑惑的——为什么失败和为什么成功了？

​	   本次安装这些问题可能弄的不是特别明白，希望下面的实验能真正了解到如何去使用hadoop和spark吧，继续加油！







### 安装参考链接：

http://dblab.xmu.edu.cn/blog/install-hadoop-cluster/

https://zhuanlan.zhihu.com/p/266050724

https://zhuanlan.zhihu.com/p/125107768





